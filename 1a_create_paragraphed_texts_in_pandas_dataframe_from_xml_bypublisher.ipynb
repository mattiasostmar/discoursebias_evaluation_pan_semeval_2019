{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Mattias Ã–stmar, mattiasostmar@gmail.com\n",
    "\n",
    "**Published:** 2019-03-36\n",
    "\n",
    "# Create pickled Pandas Dataframe with texts with paragraph separation\n",
    "\n",
    "This is the initial dataframe created. In `create_dataframe_unparagraphed_texts_from_validation_bypublisher_xml_file` we use a different method to extract the texts from the xml-files in [Hyperpartisan News Detection\n",
    "PAN @ SemEval 2019](https://pan.webis.de/semeval19/semeval19-web/) where `\\n\\n` between paragraphs are not preserved. \n",
    "\n",
    "In the notebook `compare_paragraphed_vs_non-paragraphed_validation_bypublisher_texts.ipynb` we compare the results from running the discoursebias software on the two different texts, one with `\\n\\n` preserved (this) and one with no paragraph separations, concluding that the bias index score from both versions of the texts are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5593256\r\n",
      "drwxr-xr-x@ 10 mos  staff         320 21 Mar 13:34 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@  9 mos  staff         288 21 Mar 13:54 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 mos  staff        6148 21 Mar 13:34 .DS_Store\r\n",
      "-rw-r--r--@  1 mos  staff        2101 21 Mar 09:51 article.xsd\r\n",
      "-rw-rw-r--@  1 mos  staff     2718431 16 Nov 15:23 articles-training-byarticle-20181122.xml\r\n",
      "-rw-rw-r--@  1 mos  staff  2717573604 22 Nov 01:17 articles-training-bypublisher-20181122.xml\r\n",
      "-rw-rw-r--@  1 mos  staff      111875 16 Nov 15:25 ground-truth-training-byarticle-20181122.xml\r\n",
      "-rw-rw-r--@  1 mos  staff   104765374 22 Nov 00:13 ground-truth-training-bypublisher-20181122.xml\r\n",
      "-rw-rw-r--@  1 mos  staff    25504989 22 Nov 00:12 ground-truth-validation-bypublisher-20181122.xml\r\n",
      "-rw-r--r--@  1 mos  staff        1628 21 Mar 09:52 ground-truth.xsd\r\n"
     ]
    }
   ],
   "source": [
    "# Show the content of subdirectory data/ which should contain the uncompressed dataset\n",
    "!ls -la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code reads through all the articles in the xml-file `articles-validation-bypublisher-20181122.xml`and stores them in a dataframe. I've left it in the Raw NBConvert-format so that you can inspect it. Change the cell format to `code` if you want to run it all again. The resulting dataframe can be loaded by the code cell below for inspection."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts = xmltodict.parse(open(\"data/articles-validation-bypublisher-20181122.xml\").read())\n",
    "\n",
    "ids = []\n",
    "articles = []\n",
    "\n",
    "texts_df = None\n",
    "\n",
    "for article in texts[\"articles\"][\"article\"]:\n",
    "    paras = []\n",
    "    \n",
    "    str_id = article[\"@id\"]\n",
    "    ids.append(str_id)\n",
    "    \n",
    "    title = article[\"@title\"]\n",
    "    paras.append(title)\n",
    "    #print(\"--------------------- {} ----------------\".format(str_id))\n",
    "    \n",
    "    # Remove meta-information such as image texts, urls etc.\n",
    "    text = None\n",
    "    if \"p\" in article and article[\"p\"] is not None:\n",
    "        for para in article[\"p\"]:\n",
    "            if isinstance(para, str): # E.g. removes image data of type collections.OrderedDict\n",
    "                paras.append(para)\n",
    "            text = \"\\n\\n \".join(paras)\n",
    "    else:\n",
    "        print(\"No `p` tags in article {}\".format(str_id))\n",
    "        text = article[\"#text\"]\n",
    "        if len(text) < 5:\n",
    "            print(\"Suspiciously little #text in article {}\".format(str_id))\n",
    "    articles.append(text)\n",
    "    \n",
    "    texts_df = pd.DataFrame({\"id\":ids, \"text\":articles})\n",
    "    texts_df.to_pickle(\"paragraphed_article_validation_bypublisher_20181122.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 2 columns):\n",
      "id      150000 non-null object\n",
      "text    150000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "texts_df = pd.read_pickle(\"paragraphed_article_validation_bypublisher_20181122.pickle\")\n",
    "print(texts_df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
